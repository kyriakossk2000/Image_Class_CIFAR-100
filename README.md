# Deep Learning: Mitigating Gradient Issues in VGG Models
Part of MLP course University of Edinburgh.
## Description 
Investigation into the vanishing/exploding gradient problem during the training of deep VGG networks on CIFAR100. This study explores the efficacy of **batch normalization** and **residual connections** in addressing these gradient problems, facilitating the successful training of notably deeper models.

## Key Findings
- Implemented solutions enhance training stability and enable deeper models to outperform shallower counterparts.
- Theoretical and practical aspects of batch normalization and residual connections were analyzed and implemented.
